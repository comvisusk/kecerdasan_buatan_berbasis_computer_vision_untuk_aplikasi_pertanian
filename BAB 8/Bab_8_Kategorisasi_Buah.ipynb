{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "51w9P4VdoxNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # pustaka untuk numerik\n",
        "import pandas as pd # pustaka pemrosesan data\n",
        "import matplotlib.pyplot as plt  # pustaka untuk plotting \n",
        "import seaborn as sns   #pustaka untuk visualisasi data\n",
        "import glob # pustaka untuk file path\n",
        "import cv2  # pustaka untuk algoritma visi komputer \n",
        "import os     # pustaka untuk interaksi dengan Sistem Operasi\n",
        "import sklearn   # pustaka untuk pembelajaran mesin \n",
        "\n",
        "import warnings\n",
        "# filter warning\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "ZoYCOMIlb9QY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "f0BMon3mpJgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"../dataset-buah\"))\n",
        "from subprocess import check_output\n"
      ],
      "metadata": {
        "id": "LLLp5Qc8o-t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking Training Dataset"
      ],
      "metadata": {
        "id": "_M5s4yw0pXy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1234)\n",
        "\n",
        "# nama directory untuk data pelatihan\n",
        "directory_training=\"../dataset-buah/Training/\"    \n",
        "# nama kelas dari tiap atribut buah\n",
        "kelas_training=[\"Nanas\",\"Alpukat\",\"Pisang\",\"Salak\",\"Kelapa\",\"Kiwi\",\n",
        "         \"Lemon\",\"Mangga\",\"Jeruk\"]\n",
        "# semua data pelatihan disimpan di array ini \n",
        "training_array=[]\n",
        "\n",
        "img_size=100\n",
        "for i in kelas_training:\n",
        "    path=os.path.join(directory_training,i)\n",
        "    print(\"path\",path)\n",
        "    class_num=kelas_training.index(i)\n",
        "    for img in os.listdir(path):        \n",
        "        img_array=cv2.imread(os.path.join(path,img))\n",
        "        img_array=cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
        "        training_array.append([img_array,class_num])"
      ],
      "metadata": {
        "id": "5MMk_4vNaz0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking Validation Dataset"
      ],
      "metadata": {
        "id": "LQkvaKMrplB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory_validation=\"../dataset-buah/Validation/\"\n",
        "kelas_validation=[\"Nanas\",\"Alpukat\",\"Pisang\",\"Salak\",\"Kelapa\",\"Kiwi\",\n",
        "         \"Lemon\",\"Mangga\",\"Jeruk\"]\n",
        "\n",
        "# semua data validasi disimpan di array ini \n",
        "validation_array=[]\n",
        "img_size=100\n",
        "for i in kelas_validation:\n",
        "    path=os.path.join(directory_validation,i)\n",
        "    print(\"path\",path)\n",
        "    class_num2=kelas_validation.index(i)\n",
        "    for img in os.listdir(path):\n",
        "       \n",
        "        img_array2=cv2.imread(os.path.join(path,img))\n",
        "        img_array2=cv2.cvtColor(img_array2, cv2.COLOR_BGR2RGB)\n",
        "        validation_array.append([img_array2,class_num2])"
      ],
      "metadata": {
        "id": "TtSMVz5Za8L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking All Sub-folder Dataset"
      ],
      "metadata": {
        "id": "xHHiP7y_pudd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ekstraksi fitur dari array pelatihan\n",
        "fruits_array_train = [features for features, label in training_array]\n",
        "\n",
        "# definisikan lokasi dan nama buah\n",
        "location = [[1, 500, 1150], [1500, 2000, 2500], [3000, 3500, 4000]]\n",
        "nama_buah = [\"Nanas\", \"Alpukat\", \"Pisang\", \"Salak\", \"Kelapa\", \"Kiwi\", \"Lemon\", \"Mangga\", \"Jeruk\"]\n",
        "\n",
        "# Iterasi lokasi dan nama buah\n",
        "for loc, name in zip(location, fruit_names):\n",
        "    # Buat subplot untuk setiap lokasi\n",
        "    plt.subplots(figsize=(8, 8))\n",
        "    for i, index in enumerate(loc):\n",
        "        # tambahkan subplot untuk setiap buah\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.imshow(fruits_array_train[index])\n",
        "        plt.title(name)\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "oqEJbSt1bF5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Data"
      ],
      "metadata": {
        "id": "yksxdKqAqF7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(training_array)\n",
        "\n",
        "# Pengacakan array data pelatihan dan validasi\n",
        "random.shuffle(training_array)\n",
        "random.shuffle(validation_array)\n",
        "\n",
        "# ekstraksi fitur dan label dari array yang teracak\n",
        "X_train, Y_train = zip(*training_array)\n",
        "X_test, Y_test = zip(*validation_array)\n",
        "\n",
        "# konversi list ke bentuk array Numpy\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "X_test = np.array(X_test)\n",
        "Y_test = np.array(Y_test)\n",
        "\n",
        "# Reshape dan normalisasi array\n",
        "X_train = X_train.reshape(-1, img_size, img_size, 3)\n",
        "X_train = X_train / 255\n",
        "X_test = X_test.reshape(-1, img_size, img_size, 3)\n",
        "X_test = X_test / 255\n",
        "\n",
        "print(\"shape of X_train=\", X_train.shape)\n",
        "print(\"shape of X_test=\", X_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "NIheJjGNbIgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "# konversi array ke bentuk kelas label sebagai matrix binary\n",
        "Y_train=to_categorical(Y_train,num_classes=9)\n",
        "Y_test=to_categorical(Y_test,num_classes=9)"
      ],
      "metadata": {
        "id": "LBZ8EhIFbKSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train.shape\n",
        "X_train.shape"
      ],
      "metadata": {
        "id": "bjUjysUmbKGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Dataset to Training and Validation"
      ],
      "metadata": {
        "id": "UBz4pGQsqapS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagi dataset untuk pelatihan dan validasi \n",
        "from sklearn.model_selection import  train_test_split\n",
        "x_train,x_val,y_train,y_val=train_test_split(X_train,Y_train,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "ghz7k46CbNuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Model"
      ],
      "metadata": {
        "id": "uge3OyohqpxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import (\n",
        "    Conv2D,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Flatten,\n",
        "    MaxPool2D,\n",
        ")\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# buat model neural network Convolutional \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16, (3, 3), padding=\"Same\", activation=\"relu\", input_shape=(100, 100, 3)))\n",
        "model.add(MaxPool2D((2, 2)))\n",
        "model.add(Dropout(0.35))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding=\"Same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D((2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.35))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding=\"Same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D((2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.35))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(9, activation=\"softmax\"))\n",
        "\n",
        "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
        "#Kompilasi model yang sudah dibangun \n",
        "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 18\n",
        "model.summary()  #tampilkan hasilnya"
      ],
      "metadata": {
        "id": "kMVXxmwRbPGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Model"
      ],
      "metadata": {
        "id": "Mt3ImsIsq6qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Data augmentation dapat membantu meningkatkan kemampuan generalisasi model dengan membuatnya lebih tahan terhadap variasi dalam data.\n",
        "datagen=ImageDataGenerator(featurewise_center=False, #hitung rerata nilai piksel di seluruh dataset latih,tiap citra kemudian dikurangi rerata ini.\n",
        "                           samplewise_center=False,  #hitung rerata nilai piksel hanya pada sampel tersebut, tiap citra dikurangi rerata ini\n",
        "                           featurewise_std_normalization=False, #membagi input data dengan seluruh standard deviasi\n",
        "                           samplewise_std_normalization=False,  #membagi input data dengan standard deviasi pada sampel tsb\n",
        "                          zca_whitening=False,  # mengurangi redundansi dari data yang terkorelasi\n",
        "                           rotation_range=0.5,    #putar 5 derajat\n",
        "                            zoom_range=0.5,        #zoom in-out 5%\n",
        "                           width_shift_range=0.5, #geser 5% secara horizontal\n",
        "                           height_shift_range=0.5, #geser 5% vertikal \n",
        "                           horizontal_flip=False,  # membalik citra secara horizontal\n",
        "                           vertical_flip=False, # membalik citra secara vertikal\n",
        "                           )\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# menerapkan model yang sudah dibuat ke dataset pelatihan dan validasi\n",
        "history=model.fit_generator(datagen.flow(x_train,y_train,batch_size=batch_size),epochs=epochs,\n",
        "                            validation_data=(x_val,y_val),steps_per_epoch=x_train.shape[0]//batch_size\n",
        "                           )"
      ],
      "metadata": {
        "id": "7pYEli2ZbRdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "fCc1ShoRrDH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "# simpan model Neural Network \n",
        "model.save(\"model_buah\")"
      ],
      "metadata": {
        "id": "Xd-ym_uVbTec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting Data"
      ],
      "metadata": {
        "id": "jmxUPROdrKPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot akurasi terhadap epoch \n",
        "plt.plot(history.history[\"val_accuracy\"],color=\"r\",label=\"val_acc\")\n",
        "plt.title(\"Accuracy Graph\")\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZYFKqDSDbVKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot loss terhadap epoch\n",
        "\n",
        "print(history.history.keys())\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "xepochs = [i+1 for i in range(0, len(history.history['loss']))]\n",
        "plt.figure(figsize=(5,3))\n",
        "\n",
        "# Loss\n",
        "#plt.ylim([-0.1,0.5])\n",
        "plt.plot(xepochs, history.history['loss'])\n",
        "plt.plot(xepochs, history.history['val_loss'])\n",
        "plt.xticks(xepochs)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# akurasi \n",
        "\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.plot(xepochs, history.history['accuracy'])\n",
        "plt.plot(xepochs, history.history['val_accuracy'])\n",
        "plt.xticks(xepochs)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XtiEybbIbW0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gambarkan matriks konfusi untuk data pelatihan\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#matriks konfusi\n",
        "y_pred=model.predict(x_val)\n",
        "y_pred_classes=np.argmax(y_pred,axis=1)\n",
        "y_true=np.argmax(y_val,axis=1)\n",
        "\n",
        "# hitung matriks konfusi\n",
        "conf_mat=confusion_matrix(y_true,y_pred_classes)\n",
        "\n",
        "# plot matriks konfusi\n",
        "fruit_names=[\"Nanas\",\"Alpukat\",\"Pisang\",\"Salak\",\"Kelapa\",\"Kiwi\",\n",
        "         \"Lemon\",\"Mangga\",\"Jeruk\"]\n",
        "\n",
        "f,ax=plt.subplots(figsize=(10,9))\n",
        "sns.heatmap(conf_mat,annot=True,fmt='.0f')\n",
        "ax.set_xticklabels(fruit_names)\n",
        "ax.set_yticklabels(fruit_names)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nv6PHTVDbYle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## gambarkan matriks konfusi untuk data validasi\n",
        "y_pred2=model.predict(X_test)\n",
        "y_pred_classes2=np.argmax(y_pred2,axis=1)\n",
        "y_true2=np.argmax(Y_test,axis=1)\n",
        "\n",
        "# hitung matriks konfusi\n",
        "conf_mat2=confusion_matrix(y_true2,y_pred_classes2)\n",
        "\n",
        "# plot matriks konfusi\n",
        "f,ax=plt.subplots(figsize=(10,9))\n",
        "sns.heatmap(conf_mat2,annot=True,fmt=\".0f\")\n",
        "ax.set_xticklabels(fruit_names)\n",
        "ax.set_yticklabels(fruit_names)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mI2ysNR6baSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing and Prediction"
      ],
      "metadata": {
        "id": "6jdcDf7iraBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as k\n",
        "PREDICTION_PATH=\"C:/Users/ASUS ROG/Documents/klasifikasi_buah/input/testing-prediksi\"\n",
        "# coba prediksi citra\n",
        "\n",
        "images_for_prediction = [filename for filename in sorted(os.listdir(PREDICTION_PATH)) if filename.endswith(\".jpg\")]\n",
        "\n",
        "\n",
        "for filename in images_for_prediction:\n",
        "    loaded_image = k.preprocessing.image.load_img(path=PREDICTION_PATH+'/'+filename, target_size=(img_size,img_size,3))\n",
        "    #konversi array dan resample dibagi 255\n",
        "    img_array = k.preprocessing.image.img_to_array(loaded_image) / 255.\n",
        "\n",
        "    # tambahkan dimensi \n",
        "    img_np_array = np.expand_dims(img_array, axis = 0)\n",
        "    \n",
        "    predictions = model.predict(img_np_array)\n",
        "    classidx = np.argmax(predictions[0])\n",
        "    label = nama_buah[classidx]\n",
        "\n",
        "    predictions_pct = [\"{:.2f}%\".format(prob * 100) for prob in predictions[0] ]\n",
        "    print(dict(zip(nama_buah, predictions_pct)) )\n",
        "    print(\"Prediction: %s (class %s) %s\" % (label, classidx, predictions_pct[classidx])) \n",
        "\n",
        "    plt.figure(figsize=(3,4))\n",
        "    plt.imshow(img_array)\n",
        "    plt.title(\"%s %s\" % (label, predictions_pct[classidx]))\n",
        "    plt.show()\n",
        "\n",
        "plt.gcf().clear()\n",
        "\n"
      ],
      "metadata": {
        "id": "CZPbG9fDbb-v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}